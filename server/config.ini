[feature_store]
# text2vec model path, support local relative path and huggingface model format.
# also support local path, model_path = "/path/to/your/text2vec-model"
embedding_model_path = "maidalun1020/bce-embedding-base_v1"
reranker_model_path = "maidalun1020/bce-reranker-base_v1"

# `feature_store.py` use this throttle to distinct `can_questions` and `cannot_questions`
[feature_store.reject_throttle]
en = 0.3615533853170947
zh = 0.3641814520559915

[feature_store.work_dir]
en = "source/workdir/en"
zh = "source/workdir/zh"

[llm]
enable_local = 0
enable_remote = 1

[llm.server]
# local LLM configuration
# support "internlm/internlm2-chat-7b" and "qwen/qwen-7b-chat-int8"
# support local path, for example
# local_llm_path = "/path/to/your/internlm2"

local_llm_path = "internlm/internlm2-chat-7b"
local_llm_max_text_length = 3000

# remote LLM service configuration
# support "gpt", "kimi", "deepseek", "zhipuai", "step", "internlm", "xi-api"
# support "siliconcloud", see https://siliconflow.cn/zh-cn/siliconcloud
# xi-api is chinese gpt proxy
# for internlm, see https://internlm.intern-ai.org.cn/api/document

remote_type = "kimi"
remote_api_key = "YOUR-API-KEY-HERE"
# max text length for remote LLM.
# use 128000 for kimi, 192000 for gpt/xi-api, 16000 for deepseek, 128000 for zhipuai, 40000 for internlm2
remote_llm_max_text_length = 128000
# openai API model type, support model list:
# "auto" for kimi. To save money, we auto select model name by prompt length.
# "auto" for step to save money, see https://platform.stepfun.com/
# "gpt-4-0613" for gpt/xi-api,
# "deepseek-chat" for deepseek,
# "glm-4" for zhipuai,
# "internlm2-latest" for internlm
# for example "alibaba/Qwen1.5-110B-Chat", see https://siliconflow.readme.io/reference/chat-completions-1
remote_llm_model = "auto"
# request per minute
rpm = 500