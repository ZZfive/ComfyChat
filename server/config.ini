[feature_store]
# text2vec model path, support local relative path and huggingface model format.
# also support local path, model_path = "/path/to/your/text2vec-model"
embedding_model_path = "maidalun1020/bce-embedding-base_v1"
reranker_model_path = "maidalun1020/bce-reranker-base_v1"

# `feature_store.py` use this throttle to distinct `can_questions` and `cannot_questions`
[feature_store.reject_throttle]
en = 0.3615533853170947
zh = 0.3641814520559915

[feature_store.work_dir]
en = "source/workdir/en"
zh = "source/workdir/zh"

[llm]
enable_local = 0
enable_remote = 1

[llm.server]
# local LLM configuration
# support "internlm/internlm2-chat-7b" and "qwen/qwen-7b-chat-int8"
# support local path, for example
# local_llm_path = "/path/to/your/internlm2"

local_engine = "transformers"  # 与lmdeploy二选一
local_llm_path = "internlm/internlm2-chat-7b"
local_llm_max_text_length = 3000

# remote LLM service configuration
# support "gpt", "kimi", "deepseek", "zhipuai", "step", "internlm", "xi-api"
# support "siliconcloud", see https://siliconflow.cn/zh-cn/siliconcloud
# xi-api is chinese gpt proxy
# for internlm, see https://internlm.intern-ai.org.cn/api/document

remote_type = "kimi"
remote_api_key = "YOUR-API-KEY-HERE"
# max text length for remote LLM.
# use 128000 for kimi, 192000 for gpt/xi-api, 16000 for deepseek, 128000 for zhipuai, 40000 for internlm2
remote_llm_max_text_length = 128000
# openai API model type, support model list:
# "auto" for kimi. To save money, we auto select model name by prompt length.
# "auto" for step to save money, see https://platform.stepfun.com/
# "gpt-4-0613" for gpt/xi-api,
# "deepseek-chat" for deepseek,
# "glm-4" for zhipuai,
# "internlm2-latest" for internlm
# for example "alibaba/Qwen1.5-110B-Chat", see https://siliconflow.readme.io/reference/chat-completions-1
remote_llm_model = "auto"
# request per minute
rpm = 500

[whisperx]
model = "large-v2"  # medium
download_root = "weights"

[gptsovits]
default_gpt_path = "weights/GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt"
default_sovits_path = "weights/GPT_SoVITS/pretrained_models/s2G488k.pth"
bert_path = "weights/GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large"
cnhubert_base_path = "weights/GPT_SoVITS/pretrained_models/chinese-hubert-base"
default_cut_punc = ",.;?!、，。？！；：…"

[gptsovits.gpt_path]
paimeng = "weights/GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt"
luocha = "weights/GPT_SoVITS/罗刹-e10.ckpt"
hutao = "weights/GPT_SoVITS/胡桃-e10.ckpt"
xiao = "weights/GPT_SoVITS/魈-e10.ckpt"

[gptsovits.sovits_path]
paimeng = "weights/GPT_SoVITS/pretrained_models/s2G488k.pth"
luocha = "weights/GPT_SoVITS/罗刹_e15_s450.pth"
hutao = "weights/GPT_SoVITS/胡桃_e15_s825.pth"
xiao = "weights/GPT_SoVITS/魈_e15_s780.pth"

[gptsovits.wav]
paimeng = "audio/wavs/疑问—哇，这个，还有这个…只是和史莱姆打了一场，就有这么多结论吗？.wav"
luocha = "audio/wavs/说话-行商在外，无依无靠，懂些自救的手法，心里多少有个底。.wav"
hutao = "audio/wavs/本堂主略施小计，你就败下阵来了，嘿嘿。.wav"
xiao = "audio/wavs/…你的愿望，我俱已知晓。轻策庄下，确有魔神残躯。.wav"

[gptsovits.prompt]
paimeng = "疑问—哇，这个，还有这个…只是和史莱姆打了一场，就有这么多结论吗？"
luocha = "说话-行商在外，无依无靠，懂些自救的手法，心里多少有个底。"
hutao = "本堂主略施小计，你就败下阵来了，嘿嘿。"
xiao = "…你的愿望，我俱已知晓。轻策庄下，确有魔神残躯。"

[gptsovits.language]
paimeng = "zh"
luocha = "zh"
hutao = "zh"
xiao = "zh"


[comfyui]
enable = 1  # 1开启comfyui模块，0不开启

[comfyui.server]
subdir = "visual_genrations/comfyui"
file = "visual_genrations/comfyui/main.py"
port = 8188


[comfyui.module]
design_mode = 1
lora_weight = 0.8
controlnet_num = 3
controlnet_saveimage = 1
prompt = "(best quality:1), (high quality:1), detailed/(extreme, highly, ultra/), realistic, 1girl/(beautiful, delicate, perfect/), "
negative_prompt = "(worst quality:1), (low quality:1), (normal quality:1), lowres, signature, blurry, watermark, duplicate, bad link, plump, bad anatomy, extra arms, extra digits, missing finger, bad hands, bad feet, deformed, error, mutation, text"
output_dir = "visual_genrations/comfyui/output"

[server]
server_name = "0.0.0.0"
port = 7860