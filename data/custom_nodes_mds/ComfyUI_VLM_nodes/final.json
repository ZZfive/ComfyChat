[
    {
        "messages": [
            {
                "role": "user",
                "content": "How do I use ComfyUI_VLM_nodes for Windows and Linux?"
            },
            {
                "role": "assistant",
                "content": "You need to go to the custom_nodes folder, then clone the repository using the command: cd custom_nodes && git clone https://github.com/gokayfem/ComfyUI_VLM_nodes.git"
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "How do I use ComfyUI_VLM_nodes for macOS or AMD GPUs (ROCm)?"
            },
            {
                "role": "assistant",
                "content": "You need to go to the mac branch, download the repository as zip and unzip it to the custom_nodes folder."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "What are the requirements for using VLM Nodes?"
            },
            {
                "role": "assistant",
                "content": "Python>=3.9 is necessary, and you need to put all of the files inside models/LLavacheckpoints."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "What is Structured Output node?"
            },
            {
                "role": "assistant",
                "content": "The Structured Output node is used to get structured outputs, which can be challenging through prompt engineering alone. It can extract entities, numbers, classify prompts with given classes, and generate one specific prompt."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "How is music generated from images using ComfyUI_VLM_nodes?"
            },
            {
                "role": "assistant",
                "content": "Music is generated from images using VLMs, LLMs, and AudioLDM-2. The necessary files are automatically downloaded into models/LLavacheckpoints/files_for_audioldm2."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "How can I set up the InternLV_XComposer2-VL node?"
            },
            {
                "role": "assistant",
                "content": "The InternLV_XComposer2-VL node is set up using AutoGPTQ for integration of the InternLM-XComposer2-VL model. The necessary files are automatically downloaded into models/LLavacheckpoints/files_for_internlm."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "What VLM is used for the Automatic Prompt Generation and Suggestion Nodes?"
            },
            {
                "role": "assistant",
                "content": "LLava 1.5 and 1.6 are used for the Automatic Prompt Generation and Suggestion Nodes."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "What is UForm-Gen2 Qwen used for?"
            },
            {
                "role": "assistant",
                "content": "UForm-Gen2 Qwen is used for Image Captioning and Visual Question Answering. It is a small generative vision-language model designed for speed."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "How can I use JoyTag?"
            },
            {
                "role": "assistant",
                "content": "JoyTag is used for tagging images using the Danbooru tagging schema. It will automatically download the necessary files into models/LLavacheckpoints/files_for_joytagger."
            }
        ]
    }
]