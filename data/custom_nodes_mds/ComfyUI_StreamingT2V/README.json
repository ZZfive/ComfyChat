[
    {
        "subject": "ComfyUI_StreamingT2V",
        "question": "How can I use GLM for my own NLU tasks?",
        "answer": "To use GLM for your own NLU tasks, you should implement a subclass of `DataProcessor` in [tasks/superglue/dataset.py](dataset.py) and a subclass of `PVP` in [tasks/superglue/pvp.py](pvp.py). You should also specify the We will take the RTE and ReCoRD tasks in SuperGLUE as an example."
    },
    {
        "subject": "ComfyUI_StreamingT2V",
        "question": "What is the RTE task and how does GLM process it?",
        "answer": "RTE is an NLI task in which the model is required to predict text entailment between a premise and a hypothesis. The label can be `entailment` or `not_entailmnet`. GLM predicts 'Yes' for `entailment` and 'No' for `not_entailment`. 'Yes' and 'No' are called verbalizers for `entailment` and `not_entailment` respectively."
    },
    {
        "subject": "ComfyUI_StreamingT2V",
        "question": "How does ReCoRD task differ from the RTE task?",
        "answer": "ReCoRD is a multi-choice QA task. Each example consists of a news article and a Cloze-style question about the article in which one entity is masked out. The system must predict the masked out entity from a list of possible entities in the provided passage. Unlike RTE, ReCoRD involves guessing a masked out entity from a list of candidates."
    },
    {
        "subject": "ComfyUI_StreamingT2V RteProcessor",
        "question": "What methods does the RteProcessor subclass implement?",
        "answer": "A subclass of `DataProcessor` should implement `get_train_examples`, `get_dev_examples` and `get_test_examples`, which return the examples of the train, dev, and test sets. It should also implement `get_labels` to return the list of possible labels."
    },
    {
        "subject": "ComfyUI_StreamingT2V ReCoRDPVP",
        "question": "Is `is_multi_token` True or False for the ReCoRDPVP class?",
        "answer": "For the ReCoRDPVP class, `is_multi_token` is True."
    },
    {
        "subject": "ComfyUI_StreamingT2V",
        "question": "How do I run the experiment for a new task?",
        "answer": "To run the experiment for a new task, you should create a config file and specify the evaluation metrics for the task in `DEFAULT_METRICS` of [tasks/superglue/finetune.py](finetune.py). Then you can run the experiment with `finetune_superglue.sh`."
    }
]