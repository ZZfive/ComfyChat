# LLM Chat Engine
## Documentation
- Class name: `LLMChatEngine`
- Category: `Chat Engine`
- Output node: `False`

The LLMChatEngine node facilitates interaction with a language model chat engine, allowing users to submit queries and receive responses. It abstracts the complexity of interfacing with the chat engine, providing a simple method for querying and retrieving text-based answers.
## Input types
### Required
- **`llm_index`**
    - Represents the index of the language model to be used for chatting. It is crucial for selecting the appropriate chat engine instance.
    - Comfy dtype: `LLM_INDEX`
    - Python dtype: `CustomType[LLM_INDEX]`
- **`query`**
    - The text query submitted by the user. It serves as the input for the chat engine, determining the context and content of the response.
    - Comfy dtype: `STRING`
    - Python dtype: `str`
## Output types
- **`string`**
    - Comfy dtype: `STRING`
    - The text response generated by the chat engine in reply to the user's query.
    - Python dtype: `str`
## Usage tips
- Infra type: `CPU`
- Common nodes: unknown


## Source code
```python
class LLMChatEngine:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "llm_index": ("LLM_INDEX",),
                "query": ("STRING", {"multiline": True, "dynamicPrompts": False, "placeholder": "Ask a question"}),
            },
        }

    RETURN_TYPES = ("STRING",)
    FUNCTION = "chat"
    CATEGORY = "Chat Engine"

    def chat(self, llm_index, query):
        chat_engine = llm_index.as_chat_engine()
        response = chat_engine.chat(query)
        pprint(response, indent=4)
        return (response.response,)

```
