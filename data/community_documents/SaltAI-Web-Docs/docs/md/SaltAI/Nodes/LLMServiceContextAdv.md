# LLM Service Context (Advanced)
## Documentation
- Class name: `LLMServiceContextAdv`
- Category: `SALT/Llama-Index/Context`
- Output node: `False`

This node is designed to create an advanced service context for language model operations, allowing for detailed configuration of context window, chunk size, overlap, and output settings. It facilitates the customization of language model processing by adjusting parameters such as chunk size, overlap, context window, and the number of outputs, thereby enhancing the efficiency and effectiveness of language model interactions.
## Input types
### Required
- **`llm_model`**
    - Specifies the language model to be used for generating the service context. This parameter is crucial for defining the core model around which the service context is built.
    - Comfy dtype: `LLM_MODEL`
    - Python dtype: `str`
### Optional
- **`llm_embed_model`**
    - Determines the embedding model to be used in conjunction with the main language model, providing an option for enhanced processing capabilities.
    - Comfy dtype: `LLM_EMBED`
    - Python dtype: `str`
- **`llm_node_parser`**
    - Optional parser for processing the nodes generated by the language model, allowing for further customization of the service context.
    - Comfy dtype: `LLM_NODE_PARSER`
    - Python dtype: `Optional[LLMNodeParser]`
- **`enable_chunk_overlap`**
    - Allows for the overlapping of chunks to ensure continuity and coherence in the processed data.
    - Comfy dtype: `BOOLEAN`
    - Python dtype: `bool`
- **`chunk_overlap`**
    - Specifies the amount of overlap between chunks, enhancing the connection between processed segments.
    - Comfy dtype: `INT`
    - Python dtype: `int`
- **`enable_context_window`**
    - Enables the setting of a context window size, which determines the amount of contextual information considered during processing.
    - Comfy dtype: `BOOLEAN`
    - Python dtype: `bool`
- **`context_window`**
    - Defines the size of the context window, directly influencing the model's understanding and generation capabilities.
    - Comfy dtype: `INT`
    - Python dtype: `int`
- **`enable_num_output`**
    - Activates the configuration of the number of outputs generated by the model, allowing for control over the volume of generated content.
    - Comfy dtype: `BOOLEAN`
    - Python dtype: `bool`
- **`num_output`**
    - Specifies the number of outputs to be generated, offering flexibility in the amount of content produced.
    - Comfy dtype: `INT`
    - Python dtype: `int`
- **`enable_chunk_size_limit`**
    - Allows for the imposition of a limit on chunk size, ensuring that processing remains within manageable bounds.
    - Comfy dtype: `BOOLEAN`
    - Python dtype: `bool`
- **`chunk_size_limit`**
    - Sets a maximum limit for chunk size, preventing excessive processing loads.
    - Comfy dtype: `INT`
    - Python dtype: `int`
## Output types
- **`llm_context`**
    - Comfy dtype: `LLM_CONTEXT`
    - The configured service context, ready for use in language model operations, encapsulating all specified settings and adjustments.
    - Python dtype: `ServiceContext`
## Usage tips
- Infra type: `CPU`
- Common nodes: unknown


## Source code
```python
class LLMServiceContextAdv:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "llm_model": ("LLM_MODEL",),
            },
            "optional": {
                "llm_embed_model": ("LLM_EMBED",),
                "llm_node_parser": ("LLM_NODE_PARSER",),
                "enable_chunk_overlap": ("BOOLEAN", {"default": True}),
                "chunk_overlap": ("INT", {"default": 50, "min": 0, "max": 100}),
                "enable_context_window": ("BOOLEAN", {"default": True}),
                "context_window": ("INT", {"default": 4096, "min": 2048, "max": 8192}),
                "enable_num_output": ("BOOLEAN", {"default": True}),
                "num_output": ("INT", {"default": 256, "min": 64, "max": 1024}),
                "enable_chunk_size_limit": ("BOOLEAN", {"default": True}),
                "chunk_size_limit": ("INT", {"default": 1024, "min": 512, "max": 2048}),
            },
        }

    RETURN_TYPES = ("LLM_CONTEXT",)
    RETURN_NAMES = ("llm_context",)

    FUNCTION = "context"
    CATEGORY = "SALT/Llama-Index/Context"

    def context(self, llm_model, llm_embed_model="default", llm_node_parser=None, enable_chunk_size=True, chunk_size=1024, 
                               enable_chunk_overlap=True, chunk_overlap=50, enable_context_window=True, context_window=4096, 
                               enable_num_output=True, num_output=256, enable_chunk_size_limit=True, chunk_size_limit=1024):
        prompt_helper = None
        if enable_context_window and enable_num_output:
            prompt_helper = PromptHelper(
                context_window=context_window if enable_context_window else None,
                num_output=num_output if enable_num_output else None,
                chunk_overlap_ratio=(chunk_overlap / 100.0) if enable_chunk_overlap else None,
                chunk_size_limit=chunk_size_limit if enable_chunk_size_limit else None,
            )

        service_context = ServiceContext.from_defaults(
            llm=llm_model,
            prompt_helper=prompt_helper,
            embed_model=llm_embed_model if llm_embed_model != "default" else None,
            node_parser=llm_node_parser,
            chunk_size=chunk_size if enable_chunk_size else None,
            chunk_overlap=chunk_overlap if enable_chunk_overlap else None,
        )
        return (service_context,)

```
