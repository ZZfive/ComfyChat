# LLM Complete
## Documentation
- Class name: `LLMComplete`
- Category: `SALT/Llama-Index/Querying`
- Output node: `False`

The LLMComplete node is designed to generate text completions based on a given prompt using a specified large language model (LLM). It abstracts the complexity of interacting with LLMs, providing a straightforward way to obtain rich, contextually relevant text completions.
## Input types
### Required
- **`llm_model`**
    - Specifies the large language model to be used for generating text completions. This parameter is crucial as it determines the quality and context of the generated text.
    - Comfy dtype: `LLM_MODEL`
    - Python dtype: `str`
- **`prompt`**
    - The initial text input that the LLM uses as a basis to generate completions. The quality of the prompt significantly influences the relevance and coherence of the output.
    - Comfy dtype: `STRING`
    - Python dtype: `str`
## Output types
- **`completion`**
    - Comfy dtype: `STRING`
    - The text generated by the LLM in response to the provided prompt. It represents the LLM's attempt to complete the thought or query initiated by the prompt.
    - Python dtype: `str`
## Usage tips
- Infra type: `CPU`
- Common nodes: unknown


## Source code
```python
class LLMComplete:
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "llm_model": ("LLM_MODEL", ),
                "prompt": ("STRING", {"multiline": True, "dynamicPrompts": False, "placeholder": "The circumference of the Earth is"}),
            },
        }

    RETURN_TYPES = ("STRING", )
    RETURN_NAMES = ("completion", )

    FUNCTION = "complete"
    CATEGORY = "SALT/Llama-Index/Querying"

    def complete(self, llm_model, prompt):
        response = llm_model.complete(prompt)
        pprint(response, indent=4)
        return (response.text, )

```
