[
    {
        "question": "What is the purpose of SD-Latent-Interposer?",
        "answer": "SD-Latent-Interposer is a small neural network that provides interoperability between the latents generated by different Stable Diffusion models, allowing latents from the SDXL model to be passed directly into SDv1.5 models without decoding and re-encoding using a VAE."
    },
    {
        "question": "How can SD-Latent-Interposer be installed?",
        "answer": "SD-Latent-Interposer can be installed by cloning the repo to the custom_nodes folder using the command: git clone https://github.com/city96/SD-Latent-Interposer custom_nodes/SD-Latent-Interposer. Alternatively, the comfy_latent_interposer.py file can be downloaded to the ComfyUI/custom_nodes folder."
    },
    {
        "question": "Where are the model weights for SD-Latent-Interposer hosted?",
        "answer": "The model weights for SD-Latent-Interposer are hosted on Hugging Face under the Apache2 license in the 'v4.0' subfolder."
    },
    {
        "question": "How should SD-Latent-Interposer be used in ComfyUI?",
        "answer": "SD-Latent-Interposer should be placed where a VAE decode followed by a VAE encode would normally be placed in ComfyUI. The denoise should be set appropriately to hide any artifacts while keeping the composition."
    },
    {
        "question": "What models are currently supported by SD-Latent-Interposer?",
        "answer": "SD-Latent-Interposer currently supports Stable Diffusion v1.x, SDXL, Stable Diffusion 3, Flux.1, and Stable Cascade (Stage A/B) models."
    },
    {
        "question": "How was the Interposer v4.0 model trained?",
        "answer": "The Interposer v4.0 model was trained for 50,000 steps with either batch size 128 (xl/v1) or 48 (cascade) on an RTX 3080 and a Tesla V100S."
    },
    {
        "question": "What is the purpose of the 'models' folder in the SD-Latent-Interposer directory?",
        "answer": "The 'models' folder in the SD-Latent-Interposer directory allows users to use the node completely offline by placing the required model files in the folder. The custom node will prefer local files over Hugging Face when available."
    }
]