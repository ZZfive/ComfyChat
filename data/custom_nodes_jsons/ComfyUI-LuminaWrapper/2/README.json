[
    {
        "question": "What is ComfyUI-LuminaWrapper?",
        "answer": "ComfyUI-LuminaWrapper is a custom node for ComfyUI that allows users to utilize Lumina-next text-to-image models within the ComfyUI interface."
    },
    {
        "question": "How do I install ComfyUI-LuminaWrapper?",
        "answer": "To install ComfyUI-LuminaWrapper, clone the repo into the 'custom_nodes' folder and install the dependencies using 'pip install -r requirements.txt' or the appropriate command for your setup."
    },
    {
        "question": "Why is sampling slow without 'flash_attn'?",
        "answer": "Without 'flash_attn', the attention code will fallback to torch SDP attention, which is at least twice as slow and memory hungry compared to using 'flash_attn'."
    },
    {
        "question": "What text encoder does Lumina-next use?",
        "answer": "Lumina-next uses Google's Gemma-2b large language model as its text encoder. To download it, users need to have a Huggingface account and request access."
    },
    {
        "question": "Which Lumina models are supported by ComfyUI-LuminaWrapper?",
        "answer": "ComfyUI-LuminaWrapper supports Lumina-next text-to-image models such as Lumina-Next-SFT and Lumina-Next-T2I, which are automatically downloaded to the 'ComfyUI/models/lumina' folder."
    },
    {
        "question": "Where can I find example workflows for ComfyUI-LuminaWrapper?",
        "answer": "Example workflows for ComfyUI-LuminaWrapper can be found in the 'examples' folder of the repo."
    },
    {
        "question": "What is the original repository for Lumina-T2X?",
        "answer": "The original repository for Lumina-T2X is located at https://github.com/Alpha-VLLM/Lumina-T2X."
    }
]