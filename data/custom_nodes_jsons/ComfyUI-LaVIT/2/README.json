[
    {
        "question": "What is Video-LaVIT?",
        "answer": "Video-LaVIT is a multi-modal large language model capable of both comprehending and generating videos, based on an efficient decomposed video representation."
    },
    {
        "question": "What are the capabilities of Video-LaVIT?",
        "answer": "Video-LaVIT has capabilities such as understanding image and video content, text-to-video generation, image-to-video generation, and long video generation."
    },
    {
        "question": "How does Video-LaVIT represent videos during pre-training?",
        "answer": "Video-LaVIT represents each video as keyframes and temporal motions, which are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as tokens, enabling unified generative pre-training of videos, images, and text."
    },
    {
        "question": "What datasets were used to train Video-LaVIT v1.0?",
        "answer": "Video-LaVIT v1.0 was trained only on the open-sourced WebVid-10M dataset."
    },
    {
        "question": "What is the purpose of the video detokenizer in Video-LaVIT?",
        "answer": "The video detokenizer in Video-LaVIT carefully recovers the generated tokens from the LLM to the original continuous pixel space to create various video content."
    },
    {
        "question": "How can users generate videos without watermarks using Video-LaVIT?",
        "answer": "To generate videos without watermarks, users can manually intervene the keyframe tokens by a text-to-image model like Playground-v2."
    },
    {
        "question": "What are the future plans for improving Video-LaVIT?",
        "answer": "Future plans for improving Video-LaVIT include using more video training data without watermarks, improving long video generation performance, providing the sft finetuning code, and providing the evaluation code."
    }
]