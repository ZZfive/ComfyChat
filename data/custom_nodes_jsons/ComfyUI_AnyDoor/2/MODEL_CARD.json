[
    {
        "question": "What is DINOv2?",
        "answer": "DINOv2 is a method for training Vision Transformer models without supervision, as described in the paper 'DINOv2: Learning Robust Visual Features without Supervision'."
    },
    {
        "question": "How many models are provided in the DINOv2-S/B/L/g series?",
        "answer": "There are 4 models provided: 1 ViT-g trained from scratch, and 3 ViT-S/B/L models distilled from the ViT-g."
    },
    {
        "question": "What is the embedding dimension for the ViT-S model?",
        "answer": "The embedding dimension for the ViT-S model is 384."
    },
    {
        "question": "What is the patch size used in the DINOv2 models?",
        "answer": "The DINOv2 models use a patch size of 14."
    },
    {
        "question": "Who developed the DINOv2 models?",
        "answer": "The DINOv2 models were developed by Meta AI."
    },
    {
        "question": "What is the license for the DINOv2 models?",
        "answer": "The DINOv2 models are licensed under CC-BY-NC."
    },
    {
        "question": "What are some potential biases in the DINOv2 models?",
        "answer": "Despite improvements thanks to the training method not using annotations, significant biases toward rich households from Western countries are still observed in the DINOv2 models."
    },
    {
        "question": "How much carbon was emitted during the training of the DINOv2 models?",
        "answer": "The training of the DINOv2 models emitted 7t CO2eq of carbon."
    }
]