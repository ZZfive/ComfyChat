[
    {
        "question": "What are Latent Consistency Models (LCMs)?",
        "answer": "Latent Consistency Models (LCMs) are models that enable swift inference with minimal steps on any pre-trained Latent Diffusion Models (LDMs), including Stable Diffusion. They are designed to directly predict the solution of an augmented probability flow ODE in latent space, mitigating the need for numerous iterations and allowing rapid, high-fidelity sampling."
    },
    {
        "question": "What was the inspiration behind LCMs?",
        "answer": "LCMs were inspired by Consistency Models (Song et al.)."
    },
    {
        "question": "What is the benefit of using LCMs?",
        "answer": "LCMs allow for rapid, high-fidelity sampling of images by directly predicting the solution of an augmented probability flow ODE in latent space, thus reducing the need for numerous iterations in the guided reverse diffusion process."
    },
    {
        "question": "How are LCMs trained?",
        "answer": "LCMs are efficiently distilled from pre-trained classifier-free guided diffusion models. A high-quality 768 x 768 2~4-step LCM takes only 32 A100 GPU hours for training."
    },
    {
        "question": "What is Latent Consistency Fine-tuning (LCF)?",
        "answer": "Latent Consistency Fine-tuning (LCF) is a novel method that is tailored for fine-tuning LCMs on customized image datasets."
    },
    {
        "question": "How do LCMs perform on the LAION-5B-Aesthetics dataset?",
        "answer": "Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-art text-to-image generation performance with few-step inference."
    },
    {
        "question": "Who contributed to the pipelines for LCMs?",
        "answer": "The pipelines for LCMs were contributed by luosiallen, nagolinc, and dg845."
    }
]