[
    {
        "question": "What is the purpose of the CMStochasticIterativeScheduler?",
        "answer": "The CMStochasticIterativeScheduler is a scheduler introduced in the Consistency Models paper that is capable of generating good samples in one or a small number of steps, overcoming the limitation of slow generation in diffusion models."
    },
    {
        "question": "What are the main capabilities of consistency models?",
        "answer": "Consistency models can generate high quality samples by directly mapping noise to data, support fast one-step generation by design, allow multistep sampling to trade compute for sample quality, and support zero-shot data editing without requiring explicit training on tasks like image inpainting, colorization, and super-resolution."
    },
    {
        "question": "How can consistency models be trained?",
        "answer": "Consistency models can be trained either by distilling pre-trained diffusion models or as standalone generative models altogether."
    },
    {
        "question": "What are the results achieved by consistency models on standard benchmarks?",
        "answer": "Consistency models achieve a new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation, and can outperform existing one-step, non-adversarial generative models on benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256."
    },
    {
        "question": "Where can the original codebase for consistency models be found?",
        "answer": "The original codebase for consistency models can be found at openai/consistency_models on GitHub."
    },
    {
        "question": "What is the CMStochasticIterativeSchedulerOutput?",
        "answer": "The CMStochasticIterativeSchedulerOutput is an output class for the CMStochasticIterativeScheduler, which contains the prev_sample and mid_sample."
    },
    {
        "question": "What are the two main modes of training consistency models?",
        "answer": "The two main modes of training consistency models are distilling pre-trained diffusion models and training them as standalone generative models."
    }
]