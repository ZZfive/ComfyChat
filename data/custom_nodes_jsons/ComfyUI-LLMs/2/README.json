[
    {
        "question": "What is ComfyUI-LLMs?",
        "answer": "ComfyUI-LLMs is a minimalist node in ComfyUI that allows users to call various language models, including local models, through one API."
    },
    {
        "question": "Which visual models does ComfyUI-LLMs support?",
        "answer": "ComfyUI-LLMs supports gemini, glm-4-v, and qwen-v visual models, which can be used to infer cue words."
    },
    {
        "question": "How does ComfyUI-LLMs reduce the usage of tokens?",
        "answer": "By default, the seed remains fixed after each generation in ComfyUI-LLMs. Without changing the prompt words, clicking on generate will not trigger a response, thus reducing token usage."
    },
    {
        "question": "How can users generate a new response in ComfyUI-LLMs?",
        "answer": "To generate a new response in ComfyUI-LLMs, users need to change the prompt words."
    },
    {
        "question": "Where are the settings for ComfyUI-LLMs stored?",
        "answer": "The settings for ComfyUI-LLMs are stored in the 'settings.yaml' file."
    },
    {
        "question": "On which projects is ComfyUI-LLMs based?",
        "answer": "ComfyUI-LLMs is based on the secondary development of the following projects: cyberdolphin (https://github.com/whatbirdisthat/cyberdolphin), ComfyUI_GLM4Node (https://github.com/JcandZero/ComfyUI_GLM4Node), and ComfyUI-Qwen-VL-API (https://github.com/ZHO-ZHO-ZHO/ComfyUI-Qwen-VL-API)."
    },
    {
        "question": "Under which license is ComfyUI-LLMs distributed?",
        "answer": "ComfyUI-LLMs is distributed under the GPL 3 license."
    }
]