[
    {
        "question": "What is ComfyUI-YoloNasPose-Tensorrt?",
        "answer": "ComfyUI-YoloNasPose-Tensorrt is a ComfyUI Custom Node implementation of YOLO-NAS-POSE, powered by TensorRT for ultra fast pose estimation."
    },
    {
        "question": "What is the license for ComfyUI-YoloNasPose-Tensorrt?",
        "answer": "ComfyUI-YoloNasPose-Tensorrt is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license."
    },
    {
        "question": "What is the performance of YOLO-NAS-POSE-L model with FP32 precision on an H100 device?",
        "answer": "The YOLO-NAS-POSE-L model with FP32 precision achieves 105 FPS on an H100 device with a model input resolution of 640x640 and an image resolution of 1280x720."
    },
    {
        "question": "How can I install ComfyUI-YoloNasPose-Tensorrt?",
        "answer": "To install ComfyUI-YoloNasPose-Tensorrt, navigate to the ComfyUI /custom_nodes directory, clone the repository, and install the required packages using pip install -r requirements.txt."
    },
    {
        "question": "How do I build the TensorRT engine for ComfyUI-YoloNasPose-Tensorrt?",
        "answer": "To build the TensorRT engine, download an onnx model, edit the model paths in export_trt.py, run the script, and place the exported tensorrt engine inside the ComfyUI /models/tensorrt/yolo-nas-pose directory."
    },
    {
        "question": "How can I use ComfyUI-YoloNasPose-Tensorrt in ComfyUI?",
        "answer": "To use ComfyUI-YoloNasPose-Tensorrt in ComfyUI, insert the node by Right Click -> tensorrt -> Yolo Nas Pose Tensorrt and choose the appropriate engine from the dropdown."
    },
    {
        "question": "What environment has ComfyUI-YoloNasPose-Tensorrt been tested on?",
        "answer": "ComfyUI-YoloNasPose-Tensorrt has been tested on Ubuntu 22.04 LTS with Cuda 12.4, Tensorrt 10.1.0, Python 3.10, and an H100 GPU. It has not been tested on Windows but should work."
    }
]