[
    {
        "messages": [
            {
                "role": "user",
                "content": "What is ComfyUI-IF_AI_tools?"
            },
            {
                "role": "assistant",
                "content": "ComfyUI-IF_AI_tools is a set of custom nodes for ComfyUI that allows you to generate prompts using a local Large Language Model (LLM) via Ollama."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "What are some new features of ComfyUI-IF_AI_tools?"
            },
            {
                "role": "assistant",
                "content": "Some new features of ComfyUI-IF_AI_tools include endpoints for Gemini, LlamaCpp and Mistral, and the introduction of the first tool, Omost_tool."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "How can Omost via Ollama be beneficial compared to other Omost servings?"
            },
            {
                "role": "assistant",
                "content": "Omost via Ollama can be 2x to 3x faster than other Omost servings."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "What should be done if there are failed outputs while using Omost_tool?"
            },
            {
                "role": "assistant",
                "content": "In the event of failed outputs, it might help to set the temperature below 0.5 as the LLMs can become creative enough to break the response output format, but it is normally safe up to 0.8."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "What are the prerequisites for using ComfyUI-IF_AI_tools?"
            },
            {
                "role": "assistant",
                "content": "The prerequisites for using ComfyUI-IF_AI_tools include installing Ollama and optionally Kobold.cpp, Oobabooga, Llama.cpp, or LM Studio. For optional APIs, set environment variables for ANTHROPIC_API_KEY, GEMINI_API_KEY, OPENAI_API_KEY, MISTRAL_API_KEY, and GROQ_API_KEY."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "What are the steps to install ComfyUI-IF_AI_tools?"
            },
            {
                "role": "assistant",
                "content": "The steps to install ComfyUI-IF_AI_tools include installing Ollama, opening a terminal and running a command to install the model, moving the IF_AI folder, cloning the repository, and installing the requirements."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "How can users support the development of ComfyUI-IF_AI_tools?"
            },
            {
                "role": "assistant",
                "content": "Users can support the development of ComfyUI-IF_AI_tools by starring the repository on GitHub and subscribing to the Impact Frames YouTube channel."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "What is IF_prompt_MKR?"
            },
            {
                "role": "assistant",
                "content": "IF_prompt_MKR is a similar tool available for Stable Diffusion WebUI."
            }
        ]
    }
]