[
    {
        "question": "What is ComfyUI_HF_Inference?",
        "answer": "ComfyUI_HF_Inference is an unofficial module for Hugging Face's inference API, designed for augmenting ComfyUI's capabilities."
    },
    {
        "question": "What are the primary tasks that the nodes in ComfyUI_HF_Inference support?",
        "answer": "The main tasks supported include feature extraction, question answering, translation, and generation, along with image-related capabilities such as classification, object detection, segmentation, and text-to-image conversion."
    },
    {
        "question": "What model is used for image classification in ComfyUI_HF_Inference?",
        "answer": "The model used for image classification is the [google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224)."
    },
    {
        "question": "What is the purpose of exporting HF_AUTH_TOKEN?",
        "answer": "Exporting HF_AUTH_TOKEN is necessary for authentication when interacting with Hugging Face's inference API, allowing access to the various cached and custom nodes within ComfyUI."
    },
    {
        "question": "What command is used to run ComfyUI when using ComfyUI_HF_Inference?",
        "answer": "The command used to run ComfyUI is `python main.py` with the HF_AUTH_TOKEN environment variable set appropriately."
    },
    {
        "question": "What is the limitation mentioned for the inference API (serverless)?",
        "answer": "The main limitation is that it requires a model size of 10GB or less, and it can fail for various reasons depending on the specific model used."
    }
]