[
    {
        "messages": [
            {
                "role": "user",
                "content": "What is ComfyUI-ExLlama Nodes?"
            },
            {
                "role": "assistant",
                "content": "ComfyUI-ExLlama Nodes is a simple local text generator for [ComfyUI](https://github.com/comfyanonymous/ComfyUI) using [ExLlamaV2](https://github.com/turboderp/exllamav2)."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "What text models does ComfyUI-ExLlama Nodes support?"
            },
            {
                "role": "assistant",
                "content": "Only EXL2, 4-bit GPTQ and FP16 models are supported by ComfyUI-ExLlama Nodes."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "How do I install ComfyUI-ExLlama Nodes?"
            },
            {
                "role": "assistant",
                "content": "Clone the repository to `custom_nodes` and install the requirements: `cd custom_nodes; git clone https://github.com/Zuellni/ComfyUI-ExLlama-Nodes; pip install -r ComfyUI-ExLlama-Nodes/requirements.txt`."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "What are the ExLlamaV2 model-specific nodes?"
            },
            {
                "role": "assistant",
                "content": "The ExLlamaV2 model-specific nodes are the Loader, Formatter, Tokenizer, Settings, Generator, Clean, Message, Preview, Replace, and String nodes."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "How do I use a custom model with ComfyUI-ExLlama Nodes?"
            },
            {
                "role": "assistant",
                "content": "You should clone its repository or manually download all the files and place them in a folder in the `models/llm` directory. For example, if you'd like to download the 4-bit [Llama-3.1-8B-Instruct](https://huggingface.co/turboderp/Llama-3.1-8B-Instruct-exl2), you can use the command `cd models; mkdir llm; git install lfs; git clone https://huggingface.co/turboderp/Llama-3.1-8B-Instruct-exl2 -b 4.0bpw`."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "What does the_Node_Footer tooltip in ComfyUI-ExLlama Nodes developer do?"
            },
            {
                "role": "assistant",
                "content": "The Node Footer tooltip in ComfyUI-ExLlama Nodes developer provides information about the node, such as arguments and variable name differences, for quickly understanding how to use the node."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "How do I add custom nodes to ComfyUI-ExLlama Nodes?"
            },
            {
                "role": "assistant",
                "content": "To add a custom node to ComfyUI-ExLlama Nodes, you need to clone the EllieJDC repository, add your custom node to the `nodes` directory, and edit the footer_dict.json file to add the node."
            }
        ]
    },
    {
        "messages": [
            {
                "role": "user",
                "content": "What are the Community Nodes?"
            },
            {
                "role": "assistant",
                "content": "The Community Nodes are pre-trained models provided by the Syfl universe, allowing for the reuse of pre-processed sequences and explanations for various AI models."
            }
        ]
    }
]