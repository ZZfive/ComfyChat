[
    {
        "question": "What is ComfyUI Ollama?",
        "answer": "ComfyUI Ollama is a set of custom nodes for ComfyUI that allow interaction with Ollama, a large language model, using the Ollama Python client."
    },
    {
        "question": "What is required to use ComfyUI Ollama properly?",
        "answer": "To use ComfyUI Ollama properly, you need a running Ollama server that is reachable from the host running ComfyUI."
    },
    {
        "question": "How can you install ComfyUI Ollama?",
        "answer": "You can install ComfyUI Ollama using the ComfyUI Manager 'Custom Node Manager' or by cloning the repository into the 'custom_nodes' folder of your ComfyUI installation and running 'pip install -r requirements.txt'."
    },
    {
        "question": "What does the OllamaVision node do?",
        "answer": "The OllamaVision node allows you to query input images using an Ollama model with vision abilities."
    },
    {
        "question": "What is the purpose of the OllamaGenerate node?",
        "answer": "The OllamaGenerate node allows you to query an Ollama language model using a given prompt."
    },
    {
        "question": "What additional features does the OllamaGenerateAdvance node provide?",
        "answer": "The OllamaGenerateAdvance node allows you to query an Ollama language model using a given prompt with fine-tuned parameters and the ability to preserve context for generate chaining."
    },
    {
        "question": "Where can you find more information about the parameters used in the OllamaGenerateAdvance node?",
        "answer": "You can find more information about the parameters used in the OllamaGenerateAdvance node in the Ollama API docs and the model file documentation."
    }
]