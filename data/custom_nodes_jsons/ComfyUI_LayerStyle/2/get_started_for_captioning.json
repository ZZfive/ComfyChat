[
    {
        "question": "What is the purpose of the document?",
        "answer": "The document provides instructions on how to fine-tune BEiT-3 models on image captioning tasks using the COCO Captioning and NoCaps datasets."
    },
    {
        "question": "What are the two datasets mentioned for image captioning tasks?",
        "answer": "The two datasets mentioned for image captioning tasks are COCO Captioning and NoCaps."
    },
    {
        "question": "What is the recommended GPU setup for fine-tuning BEiT-3 base model on captioning tasks?",
        "answer": "The BEiT-3 base model can be fine-tuned on captioning tasks using 8 V100-32GB GPUs."
    },
    {
        "question": "What is the effective batch size in the provided example for fine-tuning BEiT-3 base model?",
        "answer": "In the provided example, the effective batch size is 8 * 32 = 256."
    },
    {
        "question": "What is the learning rate used for fine-tuning BEiT-3 base model on COCO Captioning and NoCaps datasets?",
        "answer": "The learning rate used for fine-tuning BEiT-3 base model is 4e-5 for COCO Captioning and 1e-5 for NoCaps."
    },
    {
        "question": "How can you obtain the NoCaps val and test results after evaluating the fine-tuned BEiT-3 model?",
        "answer": "After evaluating the fine-tuned BEiT-3 model, you can submit the prediction file in the output_dir to the evaluation server to obtain the NoCaps val and test results."
    },
    {
        "question": "What is the purpose of the --checkpoint_activations flag in the fine-tuning command?",
        "answer": "The --checkpoint_activations flag is used for gradient checkpointing to save GPU memory during fine-tuning."
    }
]