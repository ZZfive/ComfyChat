[
    {
        "question": "What does BEiT-3 stand for?",
        "answer": "BEiT-3 stands for BERT Pre-Training of Image Transformers, version 3."
    },
    {
        "question": "What is the primary purpose of BEiT-3?",
        "answer": "BEiT-3 is designed for pretraining on both vision and vision-language tasks, allowing it to achieve state-of-the-art performance on a variety of benchmarks."
    },
    {
        "question": "How many parameters does the BEiT3-base model have?",
        "answer": "The BEiT3-base model has 276M parameters."
    },
    {
        "question": "What is the sentencepiece model used for tokenizing texts in BEiT-3 called?",
        "answer": "The sentencepiece model used for tokenizing texts in BEiT-3 is called beit3.spm."
    },
    {
        "question": "Which package is the implementation of BEiT-3 based on?",
        "answer": "The implementation of BEiT-3 is based on the torchscale package."
    },
    {
        "question": "What was the acceptance rate of BEiT at ICLR 2022?",
        "answer": "BEiT was accepted by ICLR 2022 as Oral presentation, with an acceptance rate of 54 out of 3391."
    },
    {
        "question": "In which conference was BEiT-3 accepted?",
        "answer": "BEiT-3 was accepted by CVPR 2023."
    }
]