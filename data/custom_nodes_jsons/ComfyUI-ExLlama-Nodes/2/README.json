[
    {
        "question": "What is the purpose of ComfyUI-ExLlama-Nodes?",
        "answer": "ComfyUI-ExLlama-Nodes is a simple local text generator for ComfyUI using ExLlamaV2."
    },
    {
        "question": "What are the supported model types for ComfyUI-ExLlama-Nodes?",
        "answer": "ComfyUI-ExLlama-Nodes supports only EXL2, 4-bit GPTQ and FP16 models."
    },
    {
        "question": "How can you add your own 'llm' path for models in ComfyUI-ExLlama-Nodes?",
        "answer": "You can add your own 'llm' path to the extra_model_paths.yaml file and put the models there instead of the default location."
    },
    {
        "question": "What does the 'cache_bits' parameter in the Loader node of ComfyUI-ExLlama-Nodes do?",
        "answer": "A lower value of 'cache_bits' reduces VRAM usage but also affects generation speed and quality."
    },
    {
        "question": "What is the function of the 'Tokenizer' node in ComfyUI-ExLlama-Nodes?",
        "answer": "The 'Tokenizer' node tokenizes input text using the model's tokenizer."
    },
    {
        "question": "What does the 'stop_conditions' parameter in the Generator node of ComfyUI-ExLlama-Nodes do?",
        "answer": "The 'stop_conditions' parameter is a list of strings to stop generation on. Leaving it empty will only stop on 'eos'."
    },
    {
        "question": "What is the purpose of the 'Clean' node in the Text Nodes section of ComfyUI-ExLlama-Nodes?",
        "answer": "The 'Clean' node strips punctuation, fixes whitespace, and changes case for input text."
    }
]