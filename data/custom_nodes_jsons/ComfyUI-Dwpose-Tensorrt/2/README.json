[
    {
        "question": "What is ComfyUI-Dwpose-Tensorrt?",
        "answer": "ComfyUI-Dwpose-Tensorrt is a project that provides a Tensorrt implementation of Dwpose for ultra fast pose estimation inside ComfyUI."
    },
    {
        "question": "What is the performance of ComfyUI-Dwpose-Tensorrt on L40s device?",
        "answer": "The performance of ComfyUI-Dwpose-Tensorrt on L40s device is 20 FPS when benchmarked on FP16 engines inside ComfyUI using 1000 similar frames."
    },
    {
        "question": "What are the requirements for installing ComfyUI-Dwpose-Tensorrt?",
        "answer": "To install ComfyUI-Dwpose-Tensorrt, you need to navigate to the ComfyUI '/custom_nodes' directory, clone the repository, and install the required packages using 'pip install -r requirements.txt'."
    },
    {
        "question": "How can you build Tensorrt engines for the onnx models in ComfyUI-Dwpose-Tensorrt?",
        "answer": "To build Tensorrt engines for the onnx models in ComfyUI-Dwpose-Tensorrt, you need to download the required onnx models, run 'python export_trt.py', and place the exported engines inside ComfyUI '/models/tensorrt/dwpose' directory."
    },
    {
        "question": "How can you use ComfyUI-Dwpose-Tensorrt in ComfyUI?",
        "answer": "To use ComfyUI-Dwpose-Tensorrt in ComfyUI, you can insert the node by 'Right Click -> tensorrt -> Dwpose Tensorrt'."
    },
    {
        "question": "What environments have been tested for ComfyUI-Dwpose-Tensorrt?",
        "answer": "ComfyUI-Dwpose-Tensorrt has been tested on Ubuntu 22.04 LTS with Cuda 12.4, Tensorrt 10.2.0.post1, Python 3.10, and L40s GPU. It has not been tested on Windows but should work."
    },
    {
        "question": "Under what license is ComfyUI-Dwpose-Tensorrt distributed?",
        "answer": "ComfyUI-Dwpose-Tensorrt is distributed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license."
    }
]