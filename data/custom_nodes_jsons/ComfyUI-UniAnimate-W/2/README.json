[
    {
        "question": "What does the Align & Generate poses for UniAnimate node do?",
        "answer": "The Align & Generate poses for UniAnimate node aligns the pose sequence with the reference image and generates poses for Unianimate."
    },
    {
        "question": "What does the Animate image with UniAnimate node do?",
        "answer": "The Animate image with UniAnimate node an animates images with Unianimate."
    },
    {
        "question": "What are the minimum and recommended GPU memory requirements for running the `Animate image with UniAnimate` node?",
        "answer": "To run the `Animate image with UniAnimate` node, a minimum of ~12GB and recommended 16GB of GPU memory will be used."
    },
    {
        "question": "What happens if the first frame of the target pose sequence in the Align & Generate poses for UniAnimate node includes the entire face and full-body pose?",
        "answer": "If the first frame of the target pose sequence includes the entire face and full-body pose, it will result in more accurate estimations and better video generation results."
    },
    {
        "question": "What is the purpose of the Embed & Shift keyframes for UniAnimate node?",
        "answer": "The Embed & Shift keyframes for UniAnimate node is used for generating slots for pseudosequence inputs and integrating with modelscope to apply the unit transformation during inference."
    },
    {
        "question": "How can a user manually align and generate poses for UniAnimate using ComfyUI?",
        "answer": "A user can manually align and generate poses for UniAnimate using ComfyUI by uploading a picture/video from the 'assets' folder, selecting the 'Align & Generate poses for UniAnimate' node, and running the workflow."
    },
    {
        "question": "What is the format and purpose of the Ana Graham video model?",
        "answer": "The Ana Graham video model is used for generating videos for sound using a pseudo-one-to-one mapping protocol. It supports 24 channels and has a format that allows for expressions such as clip, clip1, clip2, etc. It can generate 12,192 frames per step, 3840 frames per step, or 307200 frames in total. It can either be fine-tuned with a custom model or resume training if a random seed is set."
    }
]