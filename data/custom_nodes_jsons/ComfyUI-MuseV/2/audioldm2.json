[
    {
        "question": "What is AudioLDM 2?",
        "answer": "AudioLDM 2 is a text-to-audio latent diffusion model that learns continuous audio representations from text embeddings to generate text-conditional sound effects, human speech and music."
    },
    {
        "question": "What are the two text encoder models used in AudioLDM 2?",
        "answer": "The two text encoder models used in AudioLDM 2 are the text-branch of CLAP and the encoder of Flan-T5."
    },
    {
        "question": "What is the purpose of AudioLDM2ProjectionModel in AudioLDM 2?",
        "answer": "In AudioLDM 2, the AudioLDM2ProjectionModel is used to project the text embeddings computed from the prompt input to a shared embedding space."
    },
    {
        "question": "How does the UNet in AudioLDM 2 differ from most other LDMs?",
        "answer": "The UNet in AudioLDM 2 is unique in that it takes two cross-attention embeddings, as opposed to one cross-attention conditioning in most other LDMs."
    },
    {
        "question": "What are the three variants of AudioLDM 2 checkpoints?",
        "answer": "The three variants of AudioLDM 2 checkpoints are: audioldm2 and audioldm2-large for text-to-audio generation, and audioldm2-music trained exclusively for text-to-music generation."
    },
    {
        "question": "How can the quality of the predicted audio sample be controlled in AudioLDM 2?",
        "answer": "In AudioLDM 2, the quality of the predicted audio sample can be controlled by the num_inference_steps argument, with higher steps giving higher quality audio at the expense of slower inference."
    },
    {
        "question": "How can multiple waveforms be generated in one go using AudioLDM 2?",
        "answer": "In AudioLDM 2, multiple waveforms can be generated in one go by setting the num_waveforms_per_prompt argument to a value greater than 1."
    }
]