[
    {
        "question": "What are Consistency Models?",
        "answer": "Consistency Models are a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality."
    },
    {
        "question": "Who proposed Consistency Models?",
        "answer": "Consistency Models were proposed by Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever."
    },
    {
        "question": "What are the advantages of Consistency Models?",
        "answer": "Consistency Models support fast one-step generation, allow multistep sampling to trade compute for sample quality, and support zero-shot data editing tasks such as image inpainting, colorization, and super-resolution without requiring explicit training on these tasks."
    },
    {
        "question": "How can Consistency Models be trained?",
        "answer": "Consistency Models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether."
    },
    {
        "question": "What are the performance benchmarks for Consistency Models?",
        "answer": "Consistency Models achieve a state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. They can also outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256."
    },
    {
        "question": "Where can the original codebase for Consistency Models be found?",
        "answer": "The original codebase for Consistency Models can be found at openai/consistency_models on GitHub."
    },
    {
        "question": "Who contributed the pipeline for Consistency Models?",
        "answer": "The pipeline for Consistency Models was contributed by dg845 and ayushtues."
    }
]