[
    {
        "question": "What is Würstchen?",
        "answer": "Würstchen is a novel architecture for text-to-image synthesis that combines competitive performance with unprecedented cost-effectiveness for large-scale text-to-image diffusion models."
    },
    {
        "question": "How does Würstchen achieve cost-effectiveness in training and inference?",
        "answer": "Würstchen achieves cost-effectiveness by learning a detailed but extremely compact semantic image representation used to guide the diffusion process, which significantly reduces the computational requirements."
    },
    {
        "question": "What is the training requirement of Würstchen compared to Stable Diffusion 2.1?",
        "answer": "The training requirement of Würstchen consists of 24,602 A100-GPU hours, compared to Stable Diffusion 2.1's 200,000 GPU hours."
    },
    {
        "question": "How does Würstchen's compression method affect computational costs?",
        "answer": "By compressing data, Würstchen can reduce computational costs for both training and inference by magnitudes, as training on 1024x1024 images is way more expensive than training on 32x32 images."
    },
    {
        "question": "What are the three stages of Würstchen?",
        "answer": "Würstchen consists of 3 stages: Stage A (a VQGAN), Stage B (a Diffusion Autoencoder), and Stage C (learned in a highly compressed latent space)."
    },
    {
        "question": "What are the improvements in Würstchen v2?",
        "answer": "Würstchen v2 includes improvements such as higher resolution (1024x1024 up to 2048x2048), faster inference, multi aspect resolution sampling, and better quality."
    },
    {
        "question": "What are the limitations of Würstchen?",
        "answer": "Würstchen's limitations include lack of detail in generated images, the ability to generate images only in 128-pixel steps, inability to render correct text in images, often not achieving photorealism, and difficulty with difficult compositional prompts."
    }
]